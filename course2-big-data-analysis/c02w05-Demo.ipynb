{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.1.0\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.12 (default, Nov 19 2016 06:48:10)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "execfile(os.path.join(os.environ[\"SPARK_HOME\"], 'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "sparkSession = SparkSession.builder.enableHiveSupport().master(\"local [2]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sparkSession.read.parquet(\"/data/sample264\")\n",
    "meta = sparkSession.read.parquet(\"/data/meta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization could be done by next function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number, sum\n",
    "\n",
    "def norm(df, key1, key2, field, n): \n",
    "    \n",
    "    window = Window.partitionBy(key1).orderBy(col(field).desc())\n",
    "        \n",
    "    topsDF = df.withColumn(\"row_number\", row_number().over(window)) \\\n",
    "        .filter(col(\"row_number\") <= n) \\\n",
    "        .drop(col(\"row_number\")) \n",
    "        \n",
    "    tmpDF = topsDF.groupBy(col(key1)).agg(col(key1), sum(col(field)).alias(\"sum_\" + field))\n",
    "   \n",
    "    normalizedDF = topsDF.join(tmpDF, key1, \"inner\") \\\n",
    "        .withColumn(\"norm_\" + field, col(field) / col(\"sum_\" + field)) \\\n",
    "        .cache()\n",
    "\n",
    "    return normalizedDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, rank\n",
    "\n",
    "userTrack = data.groupBy(col(\"userId\"), col(\"trackId\")).count()\n",
    "\n",
    "userTrackNorm = norm(userTrack, \"userId\", \"trackId\", \"count\", 1000) \\\n",
    "        .withColumn(\"id\", col(\"userId\")) \\\n",
    "        .withColumn(\"id2\", col(\"trackId\")) \\\n",
    "        .withColumn(\"norm_count\", col(\"norm_count\") * 0.5) \\\n",
    "        .select(col(\"id\"), col(\"id2\"), col(\"norm_count\"))     \n",
    "\n",
    "window = Window.orderBy(col(\"norm_count\"))\n",
    "    \n",
    "userTrackList = userTrackNorm.withColumn(\"position\", rank().over(window))\\\n",
    "    .filter(col(\"position\") < 50)\\\n",
    "    .orderBy(col(\"id\"), col(\"id2\"))\\\n",
    "    .select(col(\"id\"), col(\"id2\"))\\\n",
    "    .take(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "415763 853951\n",
      "436158 889948\n",
      "586043 800288\n",
      "586043 800317\n",
      "586043 801522\n",
      "586043 804741\n",
      "586043 805880\n",
      "586043 806233\n",
      "586043 806439\n",
      "586043 807873\n",
      "586043 808328\n",
      "586043 810571\n",
      "586043 811212\n",
      "586043 811848\n",
      "586043 815635\n",
      "586043 818116\n",
      "586043 819591\n",
      "586043 821062\n",
      "586043 822375\n",
      "586043 825775\n",
      "586043 825997\n",
      "586043 826725\n",
      "586043 831955\n",
      "586043 833018\n",
      "586043 834780\n",
      "586043 834887\n",
      "586043 835312\n",
      "586043 837744\n",
      "586043 838944\n",
      "586043 842614\n",
      "586043 844606\n",
      "586043 851992\n",
      "586043 852304\n",
      "586043 852734\n",
      "586043 852863\n",
      "586043 855577\n",
      "586043 856643\n",
      "586043 858618\n",
      "586043 858992\n",
      "586043 860220\n"
     ]
    }
   ],
   "source": [
    "for val in userTrackList:\n",
    "    print \"%s %s\" % val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
